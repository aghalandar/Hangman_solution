{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Hangman Game\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9LEdV0s6cWF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from itertools import combinations\n",
        "import json\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "import secrets\n",
        "import time\n",
        "import re\n",
        "import collections\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "try:\n",
        "    from urllib.parse import parse_qs, urlencode, urlparse\n",
        "except ImportError:\n",
        "    from urlparse import parse_qs, urlparse\n",
        "    from urllib import urlencode\n",
        "\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "from utilities import *\n",
        "from model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsgDfkW1cowA",
        "outputId": "93e56c4f-2f1f-4d80-d4da-b5c2eaadccc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-gram is Done!\n",
            "3-gram is Done!\n",
            "4-gram is Done!\n",
            "5-gram is Done!\n",
            "6-gram is Done!\n",
            "Input data saved successfully.\n",
            "Target data saved successfully.\n",
            "torch.Size([29977280, 6]) torch.Size([29977280, 26])\n"
          ]
        }
      ],
      "source": [
        "# prepare the data for LSTM\n",
        "input_tensor, target_tensor = process_and_prepare_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMxocGfmiCO2",
        "outputId": "485cb3af-4166-449d-a8c1-20c73347e92a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 16.674883  [  128/29977280]\n",
            "loss: 16.267006  [128128/29977280]\n",
            "loss: 15.784765  [256128/29977280]\n",
            "loss: 15.513313  [384128/29977280]\n",
            "loss: 15.323220  [512128/29977280]\n",
            "loss: 15.203616  [640128/29977280]\n",
            "loss: 15.313610  [768128/29977280]\n",
            "loss: 15.335670  [896128/29977280]\n",
            "loss: 15.396443  [1024128/29977280]\n",
            "loss: 15.493000  [1152128/29977280]\n",
            "loss: 15.203526  [1280128/29977280]\n",
            "loss: 15.059254  [1408128/29977280]\n",
            "loss: 14.934552  [1536128/29977280]\n",
            "loss: 14.840385  [1664128/29977280]\n",
            "loss: 15.151722  [1792128/29977280]\n",
            "loss: 14.850445  [1920128/29977280]\n",
            "loss: 15.199343  [2048128/29977280]\n",
            "loss: 15.356495  [2176128/29977280]\n",
            "loss: 15.289555  [2304128/29977280]\n",
            "loss: 15.281339  [2432128/29977280]\n",
            "loss: 15.127481  [2560128/29977280]\n",
            "loss: 14.914280  [2688128/29977280]\n",
            "loss: 15.089948  [2816128/29977280]\n",
            "loss: 15.076271  [2944128/29977280]\n",
            "loss: 14.877400  [3072128/29977280]\n",
            "loss: 15.212141  [3200128/29977280]\n",
            "loss: 14.767307  [3328128/29977280]\n",
            "loss: 15.407882  [3456128/29977280]\n",
            "loss: 15.394258  [3584128/29977280]\n",
            "loss: 15.143404  [3712128/29977280]\n",
            "loss: 15.116913  [3840128/29977280]\n",
            "loss: 14.832752  [3968128/29977280]\n",
            "loss: 14.810196  [4096128/29977280]\n",
            "loss: 15.230495  [4224128/29977280]\n",
            "loss: 15.483963  [4352128/29977280]\n",
            "loss: 14.862824  [4480128/29977280]\n",
            "loss: 15.540615  [4608128/29977280]\n",
            "loss: 15.370943  [4736128/29977280]\n",
            "loss: 14.985857  [4864128/29977280]\n",
            "loss: 15.360314  [4992128/29977280]\n",
            "loss: 15.173878  [5120128/29977280]\n",
            "loss: 15.300743  [5248128/29977280]\n",
            "loss: 14.933387  [5376128/29977280]\n",
            "loss: 15.186083  [5504128/29977280]\n",
            "loss: 14.821743  [5632128/29977280]\n",
            "loss: 15.190594  [5760128/29977280]\n",
            "loss: 15.019341  [5888128/29977280]\n",
            "loss: 15.140773  [6016128/29977280]\n",
            "loss: 15.428632  [6144128/29977280]\n",
            "loss: 14.810994  [6272128/29977280]\n",
            "loss: 15.106788  [6400128/29977280]\n",
            "loss: 15.373544  [6528128/29977280]\n",
            "loss: 15.266751  [6656128/29977280]\n",
            "loss: 15.024984  [6784128/29977280]\n",
            "loss: 15.116135  [6912128/29977280]\n",
            "loss: 14.658319  [7040128/29977280]\n",
            "loss: 14.927027  [7168128/29977280]\n",
            "loss: 15.291486  [7296128/29977280]\n",
            "loss: 15.206290  [7424128/29977280]\n",
            "loss: 15.287566  [7552128/29977280]\n",
            "loss: 14.996025  [7680128/29977280]\n",
            "loss: 15.400881  [7808128/29977280]\n",
            "loss: 15.275807  [7936128/29977280]\n",
            "loss: 15.029297  [8064128/29977280]\n",
            "loss: 15.119665  [8192128/29977280]\n",
            "loss: 15.148947  [8320128/29977280]\n",
            "loss: 15.166110  [8448128/29977280]\n",
            "loss: 15.106606  [8576128/29977280]\n",
            "loss: 14.761158  [8704128/29977280]\n",
            "loss: 14.820324  [8832128/29977280]\n",
            "loss: 14.779140  [8960128/29977280]\n",
            "loss: 14.876839  [9088128/29977280]\n",
            "loss: 15.170005  [9216128/29977280]\n",
            "loss: 14.937731  [9344128/29977280]\n",
            "loss: 14.904267  [9472128/29977280]\n",
            "loss: 15.211018  [9600128/29977280]\n",
            "loss: 15.261481  [9728128/29977280]\n",
            "loss: 15.269526  [9856128/29977280]\n",
            "loss: 14.694445  [9984128/29977280]\n",
            "loss: 15.070547  [10112128/29977280]\n",
            "loss: 15.023336  [10240128/29977280]\n",
            "loss: 14.776737  [10368128/29977280]\n",
            "loss: 14.794841  [10496128/29977280]\n",
            "loss: 15.370955  [10624128/29977280]\n",
            "loss: 15.036487  [10752128/29977280]\n",
            "loss: 14.943150  [10880128/29977280]\n",
            "loss: 15.040998  [11008128/29977280]\n",
            "loss: 14.795789  [11136128/29977280]\n",
            "loss: 15.071118  [11264128/29977280]\n",
            "loss: 15.029507  [11392128/29977280]\n",
            "loss: 14.907337  [11520128/29977280]\n",
            "loss: 14.942109  [11648128/29977280]\n",
            "loss: 14.679776  [11776128/29977280]\n",
            "loss: 14.715355  [11904128/29977280]\n",
            "loss: 14.083012  [12032128/29977280]\n",
            "loss: 14.545714  [12160128/29977280]\n",
            "loss: 14.342654  [12288128/29977280]\n",
            "loss: 14.863528  [12416128/29977280]\n",
            "loss: 14.814470  [12544128/29977280]\n",
            "loss: 14.491107  [12672128/29977280]\n",
            "loss: 14.469671  [12800128/29977280]\n",
            "loss: 14.814920  [12928128/29977280]\n",
            "loss: 14.796879  [13056128/29977280]\n",
            "loss: 14.555149  [13184128/29977280]\n",
            "loss: 14.977951  [13312128/29977280]\n",
            "loss: 14.413429  [13440128/29977280]\n",
            "loss: 14.289139  [13568128/29977280]\n",
            "loss: 14.678733  [13696128/29977280]\n",
            "loss: 14.151499  [13824128/29977280]\n",
            "loss: 14.410163  [13952128/29977280]\n",
            "loss: 14.293640  [14080128/29977280]\n",
            "loss: 14.638496  [14208128/29977280]\n",
            "loss: 14.676355  [14336128/29977280]\n",
            "loss: 14.235001  [14464128/29977280]\n",
            "loss: 14.708281  [14592128/29977280]\n",
            "loss: 14.445272  [14720128/29977280]\n",
            "loss: 14.551402  [14848128/29977280]\n",
            "loss: 14.855494  [14976128/29977280]\n",
            "loss: 14.804646  [15104128/29977280]\n",
            "loss: 14.678738  [15232128/29977280]\n",
            "loss: 14.288714  [15360128/29977280]\n",
            "loss: 14.817199  [15488128/29977280]\n",
            "loss: 14.334774  [15616128/29977280]\n",
            "loss: 14.550098  [15744128/29977280]\n",
            "loss: 14.211121  [15872128/29977280]\n",
            "loss: 14.353006  [16000128/29977280]\n",
            "loss: 14.327262  [16128128/29977280]\n",
            "loss: 14.584632  [16256128/29977280]\n",
            "loss: 14.611332  [16384128/29977280]\n",
            "loss: 14.348522  [16512128/29977280]\n",
            "loss: 14.215246  [16640128/29977280]\n",
            "loss: 14.337503  [16768128/29977280]\n",
            "loss: 14.159035  [16896128/29977280]\n",
            "loss: 14.740572  [17024128/29977280]\n",
            "loss: 14.267899  [17152128/29977280]\n",
            "loss: 14.630930  [17280128/29977280]\n",
            "loss: 14.702579  [17408128/29977280]\n",
            "loss: 14.436928  [17536128/29977280]\n",
            "loss: 14.554342  [17664128/29977280]\n",
            "loss: 14.325838  [17792128/29977280]\n",
            "loss: 14.671621  [17920128/29977280]\n",
            "loss: 14.187974  [18048128/29977280]\n",
            "loss: 14.240409  [18176128/29977280]\n",
            "loss: 13.936801  [18304128/29977280]\n",
            "loss: 13.612355  [18432128/29977280]\n",
            "loss: 14.459929  [18560128/29977280]\n",
            "loss: 13.968105  [18688128/29977280]\n",
            "loss: 13.841027  [18816128/29977280]\n",
            "loss: 14.433440  [18944128/29977280]\n",
            "loss: 14.580121  [19072128/29977280]\n",
            "loss: 14.214520  [19200128/29977280]\n",
            "loss: 14.419985  [19328128/29977280]\n",
            "loss: 14.623037  [19456128/29977280]\n",
            "loss: 14.503869  [19584128/29977280]\n",
            "loss: 13.728707  [19712128/29977280]\n",
            "loss: 13.913340  [19840128/29977280]\n",
            "loss: 14.331408  [19968128/29977280]\n",
            "loss: 14.063225  [20096128/29977280]\n",
            "loss: 14.470604  [20224128/29977280]\n",
            "loss: 14.088853  [20352128/29977280]\n",
            "loss: 14.421948  [20480128/29977280]\n",
            "loss: 14.070520  [20608128/29977280]\n",
            "loss: 14.146193  [20736128/29977280]\n",
            "loss: 14.317261  [20864128/29977280]\n",
            "loss: 14.422800  [20992128/29977280]\n",
            "loss: 14.134791  [21120128/29977280]\n",
            "loss: 14.123504  [21248128/29977280]\n",
            "loss: 14.528111  [21376128/29977280]\n",
            "loss: 13.587755  [21504128/29977280]\n",
            "loss: 14.147816  [21632128/29977280]\n",
            "loss: 14.599452  [21760128/29977280]\n",
            "loss: 14.100953  [21888128/29977280]\n",
            "loss: 14.161576  [22016128/29977280]\n",
            "loss: 14.099071  [22144128/29977280]\n",
            "loss: 14.346500  [22272128/29977280]\n",
            "loss: 14.295625  [22400128/29977280]\n",
            "loss: 14.409623  [22528128/29977280]\n",
            "loss: 14.318829  [22656128/29977280]\n",
            "loss: 14.173212  [22784128/29977280]\n",
            "loss: 14.225740  [22912128/29977280]\n",
            "loss: 14.207005  [23040128/29977280]\n",
            "loss: 14.337912  [23168128/29977280]\n",
            "loss: 14.083628  [23296128/29977280]\n",
            "loss: 14.062943  [23424128/29977280]\n",
            "loss: 14.477405  [23552128/29977280]\n",
            "loss: 14.168795  [23680128/29977280]\n",
            "loss: 14.305738  [23808128/29977280]\n",
            "loss: 14.151396  [23936128/29977280]\n",
            "loss: 14.074354  [24064128/29977280]\n",
            "loss: 13.774090  [24192128/29977280]\n",
            "loss: 14.161644  [24320128/29977280]\n",
            "loss: 14.051014  [24448128/29977280]\n",
            "loss: 14.379669  [24576128/29977280]\n",
            "loss: 13.644682  [24704128/29977280]\n",
            "loss: 13.963859  [24832128/29977280]\n",
            "loss: 14.161453  [24960128/29977280]\n",
            "loss: 14.312671  [25088128/29977280]\n",
            "loss: 14.337201  [25216128/29977280]\n",
            "loss: 14.459291  [25344128/29977280]\n",
            "loss: 14.196238  [25472128/29977280]\n",
            "loss: 14.301867  [25600128/29977280]\n",
            "loss: 13.803243  [25728128/29977280]\n",
            "loss: 13.928074  [25856128/29977280]\n",
            "loss: 14.390998  [25984128/29977280]\n",
            "loss: 14.013954  [26112128/29977280]\n",
            "loss: 13.823211  [26240128/29977280]\n",
            "loss: 14.045643  [26368128/29977280]\n",
            "loss: 13.983355  [26496128/29977280]\n",
            "loss: 14.121160  [26624128/29977280]\n",
            "loss: 13.990067  [26752128/29977280]\n",
            "loss: 13.721044  [26880128/29977280]\n",
            "loss: 14.086555  [27008128/29977280]\n",
            "loss: 13.976990  [27136128/29977280]\n",
            "loss: 14.156358  [27264128/29977280]\n",
            "loss: 13.998252  [27392128/29977280]\n",
            "loss: 14.201671  [27520128/29977280]\n",
            "loss: 14.434763  [27648128/29977280]\n",
            "loss: 14.452377  [27776128/29977280]\n",
            "loss: 14.031934  [27904128/29977280]\n",
            "loss: 14.384651  [28032128/29977280]\n",
            "loss: 14.007417  [28160128/29977280]\n",
            "loss: 14.128235  [28288128/29977280]\n",
            "loss: 14.171737  [28416128/29977280]\n",
            "loss: 13.683745  [28544128/29977280]\n",
            "loss: 13.880520  [28672128/29977280]\n",
            "loss: 13.903792  [28800128/29977280]\n",
            "loss: 14.301104  [28928128/29977280]\n",
            "loss: 14.115676  [29056128/29977280]\n",
            "loss: 13.747379  [29184128/29977280]\n",
            "loss: 13.637188  [29312128/29977280]\n",
            "loss: 14.245070  [29440128/29977280]\n",
            "loss: 13.977839  [29568128/29977280]\n",
            "loss: 14.176030  [29696128/29977280]\n",
            "loss: 13.947451  [29824128/29977280]\n",
            "loss: 14.091169  [29952128/29977280]\n",
            "Test Error: \n",
            " Accuracy: 23.9%, Avg loss: 13.949177 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 13.868758  [  128/29977280]\n",
            "loss: 14.439633  [128128/29977280]\n",
            "loss: 13.573545  [256128/29977280]\n",
            "loss: 14.037987  [384128/29977280]\n",
            "loss: 14.047979  [512128/29977280]\n",
            "loss: 14.162625  [640128/29977280]\n",
            "loss: 14.146883  [768128/29977280]\n",
            "loss: 13.951013  [896128/29977280]\n",
            "loss: 13.829288  [1024128/29977280]\n",
            "loss: 13.791097  [1152128/29977280]\n",
            "loss: 13.792557  [1280128/29977280]\n",
            "loss: 13.906807  [1408128/29977280]\n",
            "loss: 14.285836  [1536128/29977280]\n",
            "loss: 13.820226  [1664128/29977280]\n",
            "loss: 13.815738  [1792128/29977280]\n",
            "loss: 13.506221  [1920128/29977280]\n",
            "loss: 14.269197  [2048128/29977280]\n",
            "loss: 13.847902  [2176128/29977280]\n",
            "loss: 13.691553  [2304128/29977280]\n",
            "loss: 14.302018  [2432128/29977280]\n",
            "loss: 13.917237  [2560128/29977280]\n",
            "loss: 13.517947  [2688128/29977280]\n",
            "loss: 13.933933  [2816128/29977280]\n",
            "loss: 13.942432  [2944128/29977280]\n",
            "loss: 13.937760  [3072128/29977280]\n",
            "loss: 13.936317  [3200128/29977280]\n",
            "loss: 14.130741  [3328128/29977280]\n",
            "loss: 13.718340  [3456128/29977280]\n",
            "loss: 13.680336  [3584128/29977280]\n",
            "loss: 14.338186  [3712128/29977280]\n",
            "loss: 13.651175  [3840128/29977280]\n",
            "loss: 13.962936  [3968128/29977280]\n",
            "loss: 13.964079  [4096128/29977280]\n",
            "loss: 13.652969  [4224128/29977280]\n",
            "loss: 13.819395  [4352128/29977280]\n",
            "loss: 14.070048  [4480128/29977280]\n",
            "loss: 13.919525  [4608128/29977280]\n",
            "loss: 13.788960  [4736128/29977280]\n",
            "loss: 13.924541  [4864128/29977280]\n",
            "loss: 13.882224  [4992128/29977280]\n",
            "loss: 13.685839  [5120128/29977280]\n",
            "loss: 14.171906  [5248128/29977280]\n",
            "loss: 13.781836  [5376128/29977280]\n",
            "loss: 14.343843  [5504128/29977280]\n",
            "loss: 13.501334  [5632128/29977280]\n",
            "loss: 13.864041  [5760128/29977280]\n",
            "loss: 13.625967  [5888128/29977280]\n",
            "loss: 13.659693  [6016128/29977280]\n",
            "loss: 13.996197  [6144128/29977280]\n",
            "loss: 14.098728  [6272128/29977280]\n",
            "loss: 13.913388  [6400128/29977280]\n",
            "loss: 13.898041  [6528128/29977280]\n",
            "loss: 13.867332  [6656128/29977280]\n",
            "loss: 14.191980  [6784128/29977280]\n",
            "loss: 14.069905  [6912128/29977280]\n",
            "loss: 14.361615  [7040128/29977280]\n",
            "loss: 13.210157  [7168128/29977280]\n",
            "loss: 14.134148  [7296128/29977280]\n",
            "loss: 13.649808  [7424128/29977280]\n",
            "loss: 13.908875  [7552128/29977280]\n",
            "loss: 13.743490  [7680128/29977280]\n",
            "loss: 14.219524  [7808128/29977280]\n",
            "loss: 13.707435  [7936128/29977280]\n",
            "loss: 13.557372  [8064128/29977280]\n",
            "loss: 13.858773  [8192128/29977280]\n",
            "loss: 13.654901  [8320128/29977280]\n",
            "loss: 13.818457  [8448128/29977280]\n",
            "loss: 13.782279  [8576128/29977280]\n",
            "loss: 14.163751  [8704128/29977280]\n",
            "loss: 13.903486  [8832128/29977280]\n",
            "loss: 13.868633  [8960128/29977280]\n",
            "loss: 13.857442  [9088128/29977280]\n",
            "loss: 14.300552  [9216128/29977280]\n",
            "loss: 13.614376  [9344128/29977280]\n",
            "loss: 13.684254  [9472128/29977280]\n",
            "loss: 13.680048  [9600128/29977280]\n",
            "loss: 14.098353  [9728128/29977280]\n",
            "loss: 13.584490  [9856128/29977280]\n",
            "loss: 14.131450  [9984128/29977280]\n",
            "loss: 13.890537  [10112128/29977280]\n",
            "loss: 13.633001  [10240128/29977280]\n",
            "loss: 13.547223  [10368128/29977280]\n",
            "loss: 13.994205  [10496128/29977280]\n",
            "loss: 13.929417  [10624128/29977280]\n",
            "loss: 13.843008  [10752128/29977280]\n",
            "loss: 13.936492  [10880128/29977280]\n",
            "loss: 13.535421  [11008128/29977280]\n",
            "loss: 14.089389  [11136128/29977280]\n",
            "loss: 13.740276  [11264128/29977280]\n",
            "loss: 13.611688  [11392128/29977280]\n",
            "loss: 13.799922  [11520128/29977280]\n",
            "loss: 13.448020  [11648128/29977280]\n",
            "loss: 13.429485  [11776128/29977280]\n",
            "loss: 14.231143  [11904128/29977280]\n",
            "loss: 13.872329  [12032128/29977280]\n",
            "loss: 14.140635  [12160128/29977280]\n",
            "loss: 14.024544  [12288128/29977280]\n",
            "loss: 14.159657  [12416128/29977280]\n",
            "loss: 13.669231  [12544128/29977280]\n",
            "loss: 13.799702  [12672128/29977280]\n",
            "loss: 13.685855  [12800128/29977280]\n",
            "loss: 13.719783  [12928128/29977280]\n",
            "loss: 14.619032  [13056128/29977280]\n",
            "loss: 13.864583  [13184128/29977280]\n",
            "loss: 13.720471  [13312128/29977280]\n",
            "loss: 13.635384  [13440128/29977280]\n",
            "loss: 13.657660  [13568128/29977280]\n",
            "loss: 13.876024  [13696128/29977280]\n",
            "loss: 13.967866  [13824128/29977280]\n",
            "loss: 13.816713  [13952128/29977280]\n",
            "loss: 13.695250  [14080128/29977280]\n",
            "loss: 13.683667  [14208128/29977280]\n",
            "loss: 13.603200  [14336128/29977280]\n",
            "loss: 13.580321  [14464128/29977280]\n",
            "loss: 13.546955  [14592128/29977280]\n",
            "loss: 13.993209  [14720128/29977280]\n",
            "loss: 13.821326  [14848128/29977280]\n",
            "loss: 13.906654  [14976128/29977280]\n",
            "loss: 13.793471  [15104128/29977280]\n",
            "loss: 14.323414  [15232128/29977280]\n",
            "loss: 14.080688  [15360128/29977280]\n",
            "loss: 13.761799  [15488128/29977280]\n",
            "loss: 13.756174  [15616128/29977280]\n",
            "loss: 13.530482  [15744128/29977280]\n",
            "loss: 13.880230  [15872128/29977280]\n",
            "loss: 13.615442  [16000128/29977280]\n",
            "loss: 13.712234  [16128128/29977280]\n",
            "loss: 13.617127  [16256128/29977280]\n",
            "loss: 13.445570  [16384128/29977280]\n",
            "loss: 13.131990  [16512128/29977280]\n",
            "loss: 13.069184  [16640128/29977280]\n",
            "loss: 13.736885  [16768128/29977280]\n",
            "loss: 13.542332  [16896128/29977280]\n",
            "loss: 13.879663  [17024128/29977280]\n",
            "loss: 13.377920  [17152128/29977280]\n",
            "loss: 13.466746  [17280128/29977280]\n",
            "loss: 13.942163  [17408128/29977280]\n",
            "loss: 14.143511  [17536128/29977280]\n",
            "loss: 13.823803  [17664128/29977280]\n",
            "loss: 13.178232  [17792128/29977280]\n",
            "loss: 13.988428  [17920128/29977280]\n",
            "loss: 13.990396  [18048128/29977280]\n",
            "loss: 13.665537  [18176128/29977280]\n",
            "loss: 14.013336  [18304128/29977280]\n",
            "loss: 13.555197  [18432128/29977280]\n",
            "loss: 13.875253  [18560128/29977280]\n",
            "loss: 13.508770  [18688128/29977280]\n",
            "loss: 13.301108  [18816128/29977280]\n",
            "loss: 13.719646  [18944128/29977280]\n",
            "loss: 13.644208  [19072128/29977280]\n",
            "loss: 13.506106  [19200128/29977280]\n",
            "loss: 13.282030  [19328128/29977280]\n",
            "loss: 13.733576  [19456128/29977280]\n",
            "loss: 13.889574  [19584128/29977280]\n",
            "loss: 13.679247  [19712128/29977280]\n",
            "loss: 13.743635  [19840128/29977280]\n",
            "loss: 13.482930  [19968128/29977280]\n",
            "loss: 13.719213  [20096128/29977280]\n",
            "loss: 13.356820  [20224128/29977280]\n",
            "loss: 13.931072  [20352128/29977280]\n",
            "loss: 13.515389  [20480128/29977280]\n",
            "loss: 13.752250  [20608128/29977280]\n",
            "loss: 13.083742  [20736128/29977280]\n",
            "loss: 13.701752  [20864128/29977280]\n",
            "loss: 13.841267  [20992128/29977280]\n",
            "loss: 13.547593  [21120128/29977280]\n",
            "loss: 13.221900  [21248128/29977280]\n",
            "loss: 13.859934  [21376128/29977280]\n",
            "loss: 13.537352  [21504128/29977280]\n",
            "loss: 13.456565  [21632128/29977280]\n",
            "loss: 13.127452  [21760128/29977280]\n",
            "loss: 13.885335  [21888128/29977280]\n",
            "loss: 13.418361  [22016128/29977280]\n",
            "loss: 13.668695  [22144128/29977280]\n",
            "loss: 13.448214  [22272128/29977280]\n",
            "loss: 13.837575  [22400128/29977280]\n",
            "loss: 13.461389  [22528128/29977280]\n",
            "loss: 12.909421  [22656128/29977280]\n",
            "loss: 13.794348  [22784128/29977280]\n",
            "loss: 13.927940  [22912128/29977280]\n",
            "loss: 13.454181  [23040128/29977280]\n",
            "loss: 13.558466  [23168128/29977280]\n",
            "loss: 13.647268  [23296128/29977280]\n",
            "loss: 13.851973  [23424128/29977280]\n",
            "loss: 13.354832  [23552128/29977280]\n",
            "loss: 13.341980  [23680128/29977280]\n",
            "loss: 13.273190  [23808128/29977280]\n",
            "loss: 13.926267  [23936128/29977280]\n",
            "loss: 13.204288  [24064128/29977280]\n",
            "loss: 13.750728  [24192128/29977280]\n",
            "loss: 13.630455  [24320128/29977280]\n",
            "loss: 13.528049  [24448128/29977280]\n",
            "loss: 13.240005  [24576128/29977280]\n",
            "loss: 13.102400  [24704128/29977280]\n",
            "loss: 13.677069  [24832128/29977280]\n",
            "loss: 13.396860  [24960128/29977280]\n",
            "loss: 13.725140  [25088128/29977280]\n",
            "loss: 13.162027  [25216128/29977280]\n",
            "loss: 13.116969  [25344128/29977280]\n",
            "loss: 13.428810  [25472128/29977280]\n",
            "loss: 13.801865  [25600128/29977280]\n",
            "loss: 13.438239  [25728128/29977280]\n",
            "loss: 13.737587  [25856128/29977280]\n",
            "loss: 13.461078  [25984128/29977280]\n",
            "loss: 13.051252  [26112128/29977280]\n",
            "loss: 13.441442  [26240128/29977280]\n",
            "loss: 13.378996  [26368128/29977280]\n",
            "loss: 13.698579  [26496128/29977280]\n",
            "loss: 13.195783  [26624128/29977280]\n",
            "loss: 13.247885  [26752128/29977280]\n",
            "loss: 13.589355  [26880128/29977280]\n",
            "loss: 13.378835  [27008128/29977280]\n",
            "loss: 13.780500  [27136128/29977280]\n",
            "loss: 13.267077  [27264128/29977280]\n",
            "loss: 13.474849  [27392128/29977280]\n",
            "loss: 13.403855  [27520128/29977280]\n",
            "loss: 13.551358  [27648128/29977280]\n",
            "loss: 14.000927  [27776128/29977280]\n",
            "loss: 13.415481  [27904128/29977280]\n",
            "loss: 13.425880  [28032128/29977280]\n",
            "loss: 13.127629  [28160128/29977280]\n",
            "loss: 13.638000  [28288128/29977280]\n",
            "loss: 13.562696  [28416128/29977280]\n",
            "loss: 13.796817  [28544128/29977280]\n",
            "loss: 13.480860  [28672128/29977280]\n",
            "loss: 13.161045  [28800128/29977280]\n",
            "loss: 12.945295  [28928128/29977280]\n",
            "loss: 13.234406  [29056128/29977280]\n",
            "loss: 13.325058  [29184128/29977280]\n",
            "loss: 13.740162  [29312128/29977280]\n",
            "loss: 13.273267  [29440128/29977280]\n",
            "loss: 13.657014  [29568128/29977280]\n",
            "loss: 13.202019  [29696128/29977280]\n",
            "loss: 13.702316  [29824128/29977280]\n",
            "loss: 13.589571  [29952128/29977280]\n",
            "Test Error: \n",
            " Accuracy: 19.7%, Avg loss: 13.431675 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 13.254612  [  128/29977280]\n",
            "loss: 13.661942  [128128/29977280]\n",
            "loss: 13.729284  [256128/29977280]\n",
            "loss: 13.428032  [384128/29977280]\n",
            "loss: 13.858397  [512128/29977280]\n",
            "loss: 13.530305  [640128/29977280]\n",
            "loss: 13.132339  [768128/29977280]\n",
            "loss: 13.366987  [896128/29977280]\n",
            "loss: 13.048623  [1024128/29977280]\n",
            "loss: 13.516135  [1152128/29977280]\n",
            "loss: 13.445758  [1280128/29977280]\n",
            "loss: 13.575511  [1408128/29977280]\n",
            "loss: 13.046214  [1536128/29977280]\n",
            "loss: 13.566338  [1664128/29977280]\n",
            "loss: 13.040076  [1792128/29977280]\n",
            "loss: 13.503023  [1920128/29977280]\n",
            "loss: 13.327841  [2048128/29977280]\n",
            "loss: 13.144251  [2176128/29977280]\n",
            "loss: 13.245685  [2304128/29977280]\n",
            "loss: 13.329266  [2432128/29977280]\n",
            "loss: 13.593261  [2560128/29977280]\n",
            "loss: 13.402085  [2688128/29977280]\n",
            "loss: 13.384281  [2816128/29977280]\n",
            "loss: 13.635912  [2944128/29977280]\n",
            "loss: 13.350629  [3072128/29977280]\n",
            "loss: 13.083040  [3200128/29977280]\n",
            "loss: 13.155686  [3328128/29977280]\n",
            "loss: 13.449622  [3456128/29977280]\n",
            "loss: 13.701051  [3584128/29977280]\n",
            "loss: 12.885407  [3712128/29977280]\n",
            "loss: 13.414841  [3840128/29977280]\n",
            "loss: 13.448303  [3968128/29977280]\n",
            "loss: 13.253370  [4096128/29977280]\n",
            "loss: 13.797451  [4224128/29977280]\n",
            "loss: 13.527834  [4352128/29977280]\n",
            "loss: 13.427608  [4480128/29977280]\n",
            "loss: 13.034346  [4608128/29977280]\n",
            "loss: 12.801377  [4736128/29977280]\n",
            "loss: 13.400563  [4864128/29977280]\n",
            "loss: 13.656906  [4992128/29977280]\n",
            "loss: 13.300160  [5120128/29977280]\n",
            "loss: 13.592850  [5248128/29977280]\n",
            "loss: 13.548519  [5376128/29977280]\n",
            "loss: 13.385077  [5504128/29977280]\n",
            "loss: 13.195341  [5632128/29977280]\n",
            "loss: 13.554616  [5760128/29977280]\n",
            "loss: 12.919021  [5888128/29977280]\n",
            "loss: 13.116522  [6016128/29977280]\n",
            "loss: 13.119894  [6144128/29977280]\n",
            "loss: 13.191025  [6272128/29977280]\n",
            "loss: 13.059963  [6400128/29977280]\n",
            "loss: 13.280934  [6528128/29977280]\n",
            "loss: 13.243391  [6656128/29977280]\n",
            "loss: 13.090431  [6784128/29977280]\n",
            "loss: 13.312424  [6912128/29977280]\n",
            "loss: 12.733603  [7040128/29977280]\n",
            "loss: 13.451481  [7168128/29977280]\n",
            "loss: 13.590300  [7296128/29977280]\n",
            "loss: 13.465266  [7424128/29977280]\n",
            "loss: 13.372086  [7552128/29977280]\n",
            "loss: 13.437509  [7680128/29977280]\n",
            "loss: 13.238039  [7808128/29977280]\n",
            "loss: 13.334365  [7936128/29977280]\n",
            "loss: 13.138426  [8064128/29977280]\n",
            "loss: 12.753000  [8192128/29977280]\n",
            "loss: 12.927076  [8320128/29977280]\n",
            "loss: 13.310041  [8448128/29977280]\n",
            "loss: 14.011361  [8576128/29977280]\n",
            "loss: 13.378859  [8704128/29977280]\n",
            "loss: 13.253360  [8832128/29977280]\n",
            "loss: 13.564230  [8960128/29977280]\n",
            "loss: 13.080318  [9088128/29977280]\n",
            "loss: 12.932897  [9216128/29977280]\n",
            "loss: 13.675641  [9344128/29977280]\n",
            "loss: 13.187319  [9472128/29977280]\n",
            "loss: 13.110497  [9600128/29977280]\n",
            "loss: 13.409588  [9728128/29977280]\n",
            "loss: 13.318722  [9856128/29977280]\n",
            "loss: 13.250858  [9984128/29977280]\n",
            "loss: 13.268602  [10112128/29977280]\n",
            "loss: 13.460175  [10240128/29977280]\n",
            "loss: 13.510618  [10368128/29977280]\n",
            "loss: 13.499230  [10496128/29977280]\n",
            "loss: 13.335726  [10624128/29977280]\n",
            "loss: 13.099740  [10752128/29977280]\n",
            "loss: 13.225669  [10880128/29977280]\n",
            "loss: 13.209553  [11008128/29977280]\n",
            "loss: 12.717175  [11136128/29977280]\n",
            "loss: 13.708491  [11264128/29977280]\n",
            "loss: 13.304652  [11392128/29977280]\n",
            "loss: 13.180820  [11520128/29977280]\n",
            "loss: 13.624058  [11648128/29977280]\n",
            "loss: 13.160769  [11776128/29977280]\n",
            "loss: 12.869212  [11904128/29977280]\n",
            "loss: 13.089160  [12032128/29977280]\n",
            "loss: 13.531055  [12160128/29977280]\n",
            "loss: 13.218754  [12288128/29977280]\n",
            "loss: 13.404554  [12416128/29977280]\n",
            "loss: 13.262136  [12544128/29977280]\n",
            "loss: 13.511400  [12672128/29977280]\n",
            "loss: 13.960248  [12800128/29977280]\n",
            "loss: 13.193485  [12928128/29977280]\n",
            "loss: 13.253615  [13056128/29977280]\n",
            "loss: 12.968098  [13184128/29977280]\n",
            "loss: 13.504461  [13312128/29977280]\n",
            "loss: 13.338083  [13440128/29977280]\n",
            "loss: 13.340588  [13568128/29977280]\n",
            "loss: 13.588307  [13696128/29977280]\n",
            "loss: 12.984950  [13824128/29977280]\n",
            "loss: 13.347954  [13952128/29977280]\n",
            "loss: 12.898687  [14080128/29977280]\n",
            "loss: 13.490063  [14208128/29977280]\n",
            "loss: 13.467878  [14336128/29977280]\n",
            "loss: 13.319128  [14464128/29977280]\n",
            "loss: 13.162617  [14592128/29977280]\n",
            "loss: 13.411362  [14720128/29977280]\n",
            "loss: 12.896980  [14848128/29977280]\n",
            "loss: 13.046673  [14976128/29977280]\n",
            "loss: 12.942951  [15104128/29977280]\n",
            "loss: 12.412675  [15232128/29977280]\n",
            "loss: 13.494902  [15360128/29977280]\n",
            "loss: 13.417453  [15488128/29977280]\n",
            "loss: 12.799926  [15616128/29977280]\n",
            "loss: 12.757342  [15744128/29977280]\n",
            "loss: 12.903073  [15872128/29977280]\n",
            "loss: 13.057919  [16000128/29977280]\n",
            "loss: 13.571442  [16128128/29977280]\n",
            "loss: 13.432035  [16256128/29977280]\n",
            "loss: 13.342557  [16384128/29977280]\n",
            "loss: 13.374131  [16512128/29977280]\n",
            "loss: 12.990479  [16640128/29977280]\n",
            "loss: 13.081826  [16768128/29977280]\n",
            "loss: 13.242367  [16896128/29977280]\n",
            "loss: 13.334648  [17024128/29977280]\n",
            "loss: 13.354361  [17152128/29977280]\n",
            "loss: 13.348013  [17280128/29977280]\n",
            "loss: 13.226786  [17408128/29977280]\n",
            "loss: 13.002275  [17536128/29977280]\n",
            "loss: 13.396848  [17664128/29977280]\n",
            "loss: 12.995628  [17792128/29977280]\n",
            "loss: 13.080118  [17920128/29977280]\n",
            "loss: 13.056778  [18048128/29977280]\n",
            "loss: 13.350419  [18176128/29977280]\n",
            "loss: 13.000720  [18304128/29977280]\n",
            "loss: 13.013220  [18432128/29977280]\n",
            "loss: 13.411427  [18560128/29977280]\n",
            "loss: 12.903377  [18688128/29977280]\n",
            "loss: 13.304189  [18816128/29977280]\n",
            "loss: 13.018696  [18944128/29977280]\n",
            "loss: 13.550301  [19072128/29977280]\n",
            "loss: 13.298924  [19200128/29977280]\n",
            "loss: 13.332207  [19328128/29977280]\n",
            "loss: 12.934261  [19456128/29977280]\n",
            "loss: 13.544637  [19584128/29977280]\n",
            "loss: 13.100567  [19712128/29977280]\n",
            "loss: 13.454498  [19840128/29977280]\n",
            "loss: 13.067116  [19968128/29977280]\n",
            "loss: 13.124148  [20096128/29977280]\n",
            "loss: 13.400848  [20224128/29977280]\n",
            "loss: 13.379823  [20352128/29977280]\n",
            "loss: 13.127392  [20480128/29977280]\n",
            "loss: 13.233943  [20608128/29977280]\n",
            "loss: 13.244862  [20736128/29977280]\n",
            "loss: 13.187438  [20864128/29977280]\n",
            "loss: 12.601974  [20992128/29977280]\n",
            "loss: 13.479225  [21120128/29977280]\n",
            "loss: 13.440937  [21248128/29977280]\n",
            "loss: 13.225029  [21376128/29977280]\n",
            "loss: 13.223790  [21504128/29977280]\n",
            "loss: 13.120353  [21632128/29977280]\n",
            "loss: 12.929163  [21760128/29977280]\n",
            "loss: 13.254945  [21888128/29977280]\n",
            "loss: 13.025920  [22016128/29977280]\n",
            "loss: 12.656881  [22144128/29977280]\n",
            "loss: 13.216811  [22272128/29977280]\n",
            "loss: 13.242882  [22400128/29977280]\n",
            "loss: 13.004070  [22528128/29977280]\n",
            "loss: 13.103931  [22656128/29977280]\n",
            "loss: 12.938793  [22784128/29977280]\n",
            "loss: 12.940899  [22912128/29977280]\n",
            "loss: 13.468147  [23040128/29977280]\n",
            "loss: 13.215111  [23168128/29977280]\n",
            "loss: 12.980704  [23296128/29977280]\n",
            "loss: 13.437510  [23424128/29977280]\n",
            "loss: 13.184101  [23552128/29977280]\n",
            "loss: 13.337487  [23680128/29977280]\n",
            "loss: 12.854344  [23808128/29977280]\n",
            "loss: 12.953647  [23936128/29977280]\n",
            "loss: 13.465226  [24064128/29977280]\n",
            "loss: 12.977798  [24192128/29977280]\n",
            "loss: 13.023520  [24320128/29977280]\n",
            "loss: 13.098079  [24448128/29977280]\n",
            "loss: 13.202611  [24576128/29977280]\n",
            "loss: 12.930640  [24704128/29977280]\n",
            "loss: 13.183544  [24832128/29977280]\n",
            "loss: 13.249208  [24960128/29977280]\n",
            "loss: 13.366398  [25088128/29977280]\n",
            "loss: 13.035705  [25216128/29977280]\n",
            "loss: 13.054374  [25344128/29977280]\n",
            "loss: 13.193137  [25472128/29977280]\n",
            "loss: 13.162151  [25600128/29977280]\n",
            "loss: 13.197911  [25728128/29977280]\n",
            "loss: 13.151861  [25856128/29977280]\n",
            "loss: 12.410229  [25984128/29977280]\n",
            "loss: 13.110794  [26112128/29977280]\n",
            "loss: 13.049265  [26240128/29977280]\n",
            "loss: 13.133775  [26368128/29977280]\n",
            "loss: 13.381385  [26496128/29977280]\n",
            "loss: 13.226254  [26624128/29977280]\n",
            "loss: 13.681222  [26752128/29977280]\n",
            "loss: 13.057605  [26880128/29977280]\n",
            "loss: 13.218297  [27008128/29977280]\n",
            "loss: 12.973713  [27136128/29977280]\n",
            "loss: 13.186023  [27264128/29977280]\n",
            "loss: 13.414112  [27392128/29977280]\n",
            "loss: 12.771368  [27520128/29977280]\n",
            "loss: 12.869505  [27648128/29977280]\n",
            "loss: 13.137598  [27776128/29977280]\n",
            "loss: 13.398385  [27904128/29977280]\n",
            "loss: 13.498747  [28032128/29977280]\n",
            "loss: 13.195181  [28160128/29977280]\n",
            "loss: 13.020142  [28288128/29977280]\n",
            "loss: 12.785702  [28416128/29977280]\n",
            "loss: 12.762115  [28544128/29977280]\n",
            "loss: 12.945479  [28672128/29977280]\n",
            "loss: 12.921693  [28800128/29977280]\n",
            "loss: 12.973021  [28928128/29977280]\n",
            "loss: 13.248531  [29056128/29977280]\n",
            "loss: 13.532976  [29184128/29977280]\n",
            "loss: 13.228757  [29312128/29977280]\n",
            "loss: 12.881706  [29440128/29977280]\n",
            "loss: 13.399742  [29568128/29977280]\n",
            "loss: 13.310840  [29696128/29977280]\n",
            "loss: 13.001308  [29824128/29977280]\n",
            "loss: 13.134067  [29952128/29977280]\n",
            "Test Error: \n",
            " Accuracy: 20.8%, Avg loss: 13.145944 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 12.876961  [  128/29977280]\n",
            "loss: 13.387637  [128128/29977280]\n",
            "loss: 13.161448  [256128/29977280]\n",
            "loss: 13.065113  [384128/29977280]\n",
            "loss: 13.242758  [512128/29977280]\n",
            "loss: 13.034677  [640128/29977280]\n",
            "loss: 13.336092  [768128/29977280]\n",
            "loss: 12.768873  [896128/29977280]\n",
            "loss: 13.168663  [1024128/29977280]\n",
            "loss: 13.468904  [1152128/29977280]\n",
            "loss: 13.719243  [1280128/29977280]\n",
            "loss: 13.209647  [1408128/29977280]\n",
            "loss: 13.130423  [1536128/29977280]\n",
            "loss: 12.952710  [1664128/29977280]\n",
            "loss: 13.187851  [1792128/29977280]\n",
            "loss: 13.576043  [1920128/29977280]\n",
            "loss: 13.078163  [2048128/29977280]\n",
            "loss: 13.348731  [2176128/29977280]\n",
            "loss: 13.013151  [2304128/29977280]\n",
            "loss: 12.816637  [2432128/29977280]\n",
            "loss: 12.989496  [2560128/29977280]\n",
            "loss: 13.100893  [2688128/29977280]\n",
            "loss: 13.496111  [2816128/29977280]\n",
            "loss: 13.050333  [2944128/29977280]\n",
            "loss: 13.185788  [3072128/29977280]\n",
            "loss: 13.111587  [3200128/29977280]\n",
            "loss: 13.177772  [3328128/29977280]\n",
            "loss: 13.168326  [3456128/29977280]\n",
            "loss: 13.513491  [3584128/29977280]\n",
            "loss: 13.362962  [3712128/29977280]\n",
            "loss: 13.232830  [3840128/29977280]\n",
            "loss: 13.317612  [3968128/29977280]\n",
            "loss: 13.269018  [4096128/29977280]\n",
            "loss: 12.991771  [4224128/29977280]\n",
            "loss: 13.054286  [4352128/29977280]\n",
            "loss: 13.219679  [4480128/29977280]\n",
            "loss: 13.198559  [4608128/29977280]\n",
            "loss: 12.851226  [4736128/29977280]\n",
            "loss: 12.867148  [4864128/29977280]\n",
            "loss: 13.327978  [4992128/29977280]\n",
            "loss: 12.988846  [5120128/29977280]\n",
            "loss: 13.229673  [5248128/29977280]\n",
            "loss: 13.196877  [5376128/29977280]\n",
            "loss: 12.826025  [5504128/29977280]\n",
            "loss: 12.941542  [5632128/29977280]\n",
            "loss: 12.888155  [5760128/29977280]\n",
            "loss: 12.942360  [5888128/29977280]\n",
            "loss: 12.855210  [6016128/29977280]\n",
            "loss: 13.418240  [6144128/29977280]\n",
            "loss: 13.280966  [6272128/29977280]\n",
            "loss: 13.129995  [6400128/29977280]\n",
            "loss: 13.373416  [6528128/29977280]\n",
            "loss: 13.027843  [6656128/29977280]\n",
            "loss: 13.072929  [6784128/29977280]\n",
            "loss: 12.792397  [6912128/29977280]\n",
            "loss: 12.951974  [7040128/29977280]\n",
            "loss: 13.427813  [7168128/29977280]\n",
            "loss: 13.168685  [7296128/29977280]\n",
            "loss: 13.551907  [7424128/29977280]\n",
            "loss: 13.062765  [7552128/29977280]\n",
            "loss: 13.745226  [7680128/29977280]\n",
            "loss: 12.908726  [7808128/29977280]\n",
            "loss: 13.203561  [7936128/29977280]\n",
            "loss: 12.809498  [8064128/29977280]\n",
            "loss: 13.036434  [8192128/29977280]\n",
            "loss: 12.913918  [8320128/29977280]\n",
            "loss: 13.043556  [8448128/29977280]\n",
            "loss: 12.790388  [8576128/29977280]\n",
            "loss: 13.065558  [8704128/29977280]\n",
            "loss: 13.304705  [8832128/29977280]\n",
            "loss: 13.256136  [8960128/29977280]\n",
            "loss: 12.861101  [9088128/29977280]\n",
            "loss: 13.139678  [9216128/29977280]\n",
            "loss: 13.212029  [9344128/29977280]\n",
            "loss: 13.448708  [9472128/29977280]\n",
            "loss: 12.984504  [9600128/29977280]\n",
            "loss: 12.794044  [9728128/29977280]\n",
            "loss: 13.155741  [9856128/29977280]\n",
            "loss: 13.073296  [9984128/29977280]\n",
            "loss: 13.007467  [10112128/29977280]\n",
            "loss: 12.670006  [10240128/29977280]\n",
            "loss: 12.926893  [10368128/29977280]\n",
            "loss: 13.167110  [10496128/29977280]\n",
            "loss: 13.353097  [10624128/29977280]\n",
            "loss: 13.415975  [10752128/29977280]\n",
            "loss: 13.288985  [10880128/29977280]\n",
            "loss: 12.643038  [11008128/29977280]\n",
            "loss: 13.505550  [11136128/29977280]\n",
            "loss: 12.887094  [11264128/29977280]\n",
            "loss: 13.357078  [11392128/29977280]\n",
            "loss: 13.170141  [11520128/29977280]\n",
            "loss: 12.609058  [11648128/29977280]\n",
            "loss: 13.257657  [11776128/29977280]\n",
            "loss: 13.196781  [11904128/29977280]\n",
            "loss: 13.212637  [12032128/29977280]\n",
            "loss: 13.186249  [12160128/29977280]\n",
            "loss: 13.271332  [12288128/29977280]\n",
            "loss: 13.023058  [12416128/29977280]\n",
            "loss: 13.368040  [12544128/29977280]\n",
            "loss: 12.913331  [12672128/29977280]\n",
            "loss: 13.278172  [12800128/29977280]\n",
            "loss: 12.940693  [12928128/29977280]\n",
            "loss: 12.933032  [13056128/29977280]\n",
            "loss: 12.820210  [13184128/29977280]\n",
            "loss: 12.737684  [13312128/29977280]\n",
            "loss: 13.345825  [13440128/29977280]\n",
            "loss: 13.077558  [13568128/29977280]\n",
            "loss: 12.806358  [13696128/29977280]\n",
            "loss: 13.172792  [13824128/29977280]\n",
            "loss: 13.050339  [13952128/29977280]\n",
            "loss: 13.269297  [14080128/29977280]\n",
            "loss: 12.935214  [14208128/29977280]\n",
            "loss: 13.204284  [14336128/29977280]\n",
            "loss: 13.345963  [14464128/29977280]\n",
            "loss: 12.801341  [14592128/29977280]\n",
            "loss: 13.379663  [14720128/29977280]\n",
            "loss: 13.151965  [14848128/29977280]\n",
            "loss: 13.136226  [14976128/29977280]\n",
            "loss: 13.621816  [15104128/29977280]\n",
            "loss: 12.935811  [15232128/29977280]\n",
            "loss: 13.123062  [15360128/29977280]\n",
            "loss: 13.086380  [15488128/29977280]\n",
            "loss: 13.023127  [15616128/29977280]\n",
            "loss: 13.227369  [15744128/29977280]\n",
            "loss: 13.176523  [15872128/29977280]\n",
            "loss: 13.184915  [16000128/29977280]\n",
            "loss: 13.450882  [16128128/29977280]\n",
            "loss: 13.335388  [16256128/29977280]\n",
            "loss: 13.548723  [16384128/29977280]\n",
            "loss: 12.927338  [16512128/29977280]\n",
            "loss: 13.042594  [16640128/29977280]\n",
            "loss: 13.646380  [16768128/29977280]\n",
            "loss: 13.010151  [16896128/29977280]\n",
            "loss: 13.345964  [17024128/29977280]\n",
            "loss: 12.853294  [17152128/29977280]\n",
            "loss: 13.014596  [17280128/29977280]\n",
            "loss: 12.833393  [17408128/29977280]\n",
            "loss: 13.099837  [17536128/29977280]\n",
            "loss: 13.059311  [17664128/29977280]\n",
            "loss: 13.105257  [17792128/29977280]\n",
            "loss: 13.306511  [17920128/29977280]\n",
            "loss: 13.042502  [18048128/29977280]\n",
            "loss: 12.860928  [18176128/29977280]\n",
            "loss: 12.592482  [18304128/29977280]\n",
            "loss: 13.355620  [18432128/29977280]\n",
            "loss: 12.966322  [18560128/29977280]\n",
            "loss: 13.143586  [18688128/29977280]\n",
            "loss: 12.635900  [18816128/29977280]\n",
            "loss: 13.059024  [18944128/29977280]\n",
            "loss: 13.086258  [19072128/29977280]\n",
            "loss: 13.230074  [19200128/29977280]\n",
            "loss: 12.806697  [19328128/29977280]\n",
            "loss: 12.565238  [19456128/29977280]\n",
            "loss: 13.147287  [19584128/29977280]\n",
            "loss: 13.030238  [19712128/29977280]\n",
            "loss: 13.068440  [19840128/29977280]\n",
            "loss: 13.410128  [19968128/29977280]\n",
            "loss: 13.293880  [20096128/29977280]\n",
            "loss: 12.935440  [20224128/29977280]\n",
            "loss: 12.769384  [20352128/29977280]\n",
            "loss: 13.075616  [20480128/29977280]\n",
            "loss: 12.961865  [20608128/29977280]\n",
            "loss: 13.079447  [20736128/29977280]\n",
            "loss: 13.421824  [20864128/29977280]\n",
            "loss: 12.446021  [20992128/29977280]\n",
            "loss: 12.822504  [21120128/29977280]\n",
            "loss: 13.368281  [21248128/29977280]\n",
            "loss: 13.335515  [21376128/29977280]\n",
            "loss: 13.388988  [21504128/29977280]\n",
            "loss: 13.154426  [21632128/29977280]\n",
            "loss: 13.293542  [21760128/29977280]\n",
            "loss: 13.011593  [21888128/29977280]\n",
            "loss: 13.188510  [22016128/29977280]\n",
            "loss: 12.873221  [22144128/29977280]\n",
            "loss: 12.723022  [22272128/29977280]\n",
            "loss: 13.133977  [22400128/29977280]\n",
            "loss: 13.310049  [22528128/29977280]\n",
            "loss: 13.296200  [22656128/29977280]\n",
            "loss: 12.902091  [22784128/29977280]\n",
            "loss: 13.536996  [22912128/29977280]\n",
            "loss: 13.141333  [23040128/29977280]\n",
            "loss: 13.027423  [23168128/29977280]\n",
            "loss: 12.858582  [23296128/29977280]\n",
            "loss: 13.147397  [23424128/29977280]\n",
            "loss: 13.163925  [23552128/29977280]\n",
            "loss: 13.327598  [23680128/29977280]\n",
            "loss: 12.754247  [23808128/29977280]\n",
            "loss: 13.539165  [23936128/29977280]\n",
            "loss: 13.400393  [24064128/29977280]\n",
            "loss: 13.281981  [24192128/29977280]\n",
            "loss: 12.928717  [24320128/29977280]\n",
            "loss: 13.302913  [24448128/29977280]\n",
            "loss: 12.986933  [24576128/29977280]\n",
            "loss: 13.005262  [24704128/29977280]\n",
            "loss: 13.334915  [24832128/29977280]\n",
            "loss: 12.560995  [24960128/29977280]\n",
            "loss: 13.073765  [25088128/29977280]\n",
            "loss: 13.273204  [25216128/29977280]\n",
            "loss: 12.963428  [25344128/29977280]\n",
            "loss: 13.128194  [25472128/29977280]\n",
            "loss: 13.769484  [25600128/29977280]\n",
            "loss: 13.111113  [25728128/29977280]\n",
            "loss: 12.904337  [25856128/29977280]\n",
            "loss: 12.986727  [25984128/29977280]\n",
            "loss: 12.986694  [26112128/29977280]\n",
            "loss: 13.140230  [26240128/29977280]\n",
            "loss: 13.320045  [26368128/29977280]\n",
            "loss: 13.603521  [26496128/29977280]\n",
            "loss: 12.940399  [26624128/29977280]\n",
            "loss: 13.028902  [26752128/29977280]\n",
            "loss: 12.464767  [26880128/29977280]\n",
            "loss: 12.928094  [27008128/29977280]\n",
            "loss: 13.101212  [27136128/29977280]\n",
            "loss: 13.090676  [27264128/29977280]\n",
            "loss: 13.043032  [27392128/29977280]\n",
            "loss: 13.244016  [27520128/29977280]\n",
            "loss: 13.112069  [27648128/29977280]\n",
            "loss: 13.199571  [27776128/29977280]\n",
            "loss: 12.837391  [27904128/29977280]\n",
            "loss: 13.072380  [28032128/29977280]\n",
            "loss: 13.011230  [28160128/29977280]\n",
            "loss: 13.529065  [28288128/29977280]\n",
            "loss: 12.830932  [28416128/29977280]\n",
            "loss: 13.099937  [28544128/29977280]\n",
            "loss: 13.198567  [28672128/29977280]\n",
            "loss: 13.324387  [28800128/29977280]\n",
            "loss: 12.670379  [28928128/29977280]\n",
            "loss: 13.016233  [29056128/29977280]\n",
            "loss: 13.477020  [29184128/29977280]\n",
            "loss: 13.036146  [29312128/29977280]\n",
            "loss: 12.791451  [29440128/29977280]\n",
            "loss: 13.137715  [29568128/29977280]\n",
            "loss: 13.324851  [29696128/29977280]\n",
            "loss: 13.189075  [29824128/29977280]\n",
            "loss: 12.721285  [29952128/29977280]\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "train_model(input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7qFgX38Djy8"
      },
      "outputs": [],
      "source": [
        "def encode_input_for_prediction(masked_word):\n",
        "    \"\"\"\n",
        "    Encode the masked word.\n",
        "\n",
        "    Parameters:\n",
        "    - masked_word (str): The word with masked characters.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Encoded input tensor for prediction.\n",
        "    \"\"\"\n",
        "    # Create a character-to-index mapping and an underscore placeholder\n",
        "    char_to_index, _ = create_char_mapping()\n",
        "\n",
        "    # Encode the masked word using the char_to_index mapping\n",
        "    input_data = [encode_input(masked_word)]\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
        "    return input_tensor\n",
        "\n",
        "\n",
        "def extract_ngrams(word):\n",
        "    \"\"\"\n",
        "    Make all possible n-grams from the word with at least one underscore.\n",
        "\n",
        "    Parameters:\n",
        "    - word (str): The input word.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of unique n-grams.\n",
        "    \"\"\"\n",
        "    ngrams = set()\n",
        "\n",
        "    # Iterate over different n-gram lengths (2, 3, 4, 5, 6)\n",
        "    for n in range(2, 7):\n",
        "        # Extract n-grams from the word\n",
        "        for i in range(len(word) - n + 1):\n",
        "            ngram = word[i:i + n]\n",
        "\n",
        "            # Check if the n-gram contains at least one alphabet and one underscore\n",
        "            if any(char.isalpha() for char in ngram) and '_' in ngram:\n",
        "                # If the n-gram is shorter than 6, pad it with asterisks\n",
        "                ngram = ngram.ljust(6, '*')\n",
        "\n",
        "                # Ensure the n-gram is of length 6\n",
        "                ngram = ngram[:6]\n",
        "\n",
        "                # Add the n-gram to the set\n",
        "                ngrams.add(ngram)\n",
        "\n",
        "    return list(ngrams)\n",
        "\n",
        "def encode_ngram(ngram, char_to_index):\n",
        "    \"\"\"\n",
        "    Encode a given n-gram using a character-to-index mapping.\n",
        "\n",
        "    Parameters:\n",
        "    - ngram (str): The input n-gram.\n",
        "    - char_to_index (dict): Character-to-index mapping.\n",
        "\n",
        "    Returns:\n",
        "    - list: Encoded n-gram.\n",
        "    \"\"\"\n",
        "    # Ensure the n-gram is of length 6\n",
        "    ngram = ngram[:6]\n",
        "\n",
        "    # Encode each character in the n-gram using char_to_index mapping\n",
        "    encoded_ngram = [char_to_index[char] for char in ngram]\n",
        "\n",
        "    return encoded_ngram\n",
        "\n",
        "def get_sorted_letters(new_dictionary, guessed_letters):\n",
        "    \"\"\"\n",
        "    Get sorted letters based on their frequency in the new dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - new_dictionary (list): List of possible words.\n",
        "    - guessed_letters (list): List of guessed letters.\n",
        "\n",
        "    Returns:\n",
        "    - list: Sorted letters based on frequency, excluding guessed letters.\n",
        "    \"\"\"\n",
        "    full_dict_string = \"\".join(new_dictionary)\n",
        "\n",
        "    # Count the occurrences of each letter\n",
        "    c = collections.Counter(full_dict_string)\n",
        "\n",
        "    sorted_letter_count = c.most_common()\n",
        "\n",
        "    # Filter out guessed letters\n",
        "    remaining_sorted_letters = [item for item in sorted_letter_count if item[0] not in guessed_letters]\n",
        "\n",
        "    return remaining_sorted_letters\n",
        "\n",
        "def func(new_dictionary):\n",
        "    \"\"\"\n",
        "    Count the occurrences of each letter in the new dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - new_dictionary (list): List of possible words.\n",
        "\n",
        "    Returns:\n",
        "    - collections.Counter: Count of occurrences for each letter.\n",
        "    \"\"\"\n",
        "    dictx = collections.Counter()\n",
        "    for words in new_dictionary:\n",
        "        temp = collections.Counter(words)\n",
        "        for i in temp:\n",
        "            temp[i] = 1\n",
        "            dictx = dictx + temp\n",
        "    return dictx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dchQERaVHcpe"
      },
      "outputs": [],
      "source": [
        "class HangmanAPI(object):\n",
        "    def __init__(self, access_token=None, session=None, timeout=None):\n",
        "        self.hangman_url = self.determine_hangman_url()\n",
        "        self.access_token = access_token\n",
        "        self.session = session or requests.Session()\n",
        "        self.timeout = timeout\n",
        "        self.guessed_letters = []\n",
        "        full_dictionary_location = \"/content/words_250000_train.txt\"\n",
        "        self.full_dictionary = self.build_dictionary(full_dictionary_location)\n",
        "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
        "        self.current_dictionary = []\n",
        "        self.tries_remains  = 6\n",
        "        self.model = NeuralNetwork()\n",
        "        self.model.load_state_dict(torch.load(\"/content/lstm_ngram_2.pt\"))\n",
        "        self.ngram_used = set()\n",
        "        self.LSTM_guess = 0\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def determine_hangman_url():\n",
        "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
        "\n",
        "        data = {link: 0 for link in links}\n",
        "\n",
        "        for link in links:\n",
        "\n",
        "            requests.get(link)\n",
        "\n",
        "            for i in range(10):\n",
        "                s = time.time()\n",
        "                requests.get(link)\n",
        "                data[link] = time.time() - s\n",
        "\n",
        "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
        "        link += '/trexsim/hangman'\n",
        "        return link\n",
        "\n",
        "\n",
        "    def predicted_letter_lstm(self, masked_word):\n",
        "        \"\"\"\n",
        "        Predict the next letter using the LSTM model based on the masked word.\n",
        "\n",
        "        Parameters:\n",
        "        - masked_word (str): The word with masked characters.\n",
        "\n",
        "        Returns:\n",
        "        - list: Predicted letters sorted by probability.\n",
        "        \"\"\"\n",
        "        # Create character mappings\n",
        "        char_to_index, int_to_char = create_char_mapping()\n",
        "\n",
        "        # Extract unique n-grams with at least one alphabet\n",
        "        ngrams = extract_ngrams(masked_word)\n",
        "\n",
        "        # Initialize an empty dictionary to store accumulated probabilities\n",
        "        accumulated_probabilities = {}\n",
        "\n",
        "        # Traverse over n-grams\n",
        "        for ngram in ngrams:\n",
        "            # Encode the n-gram\n",
        "            input_tensor_for_prediction = encode_ngram(ngram, char_to_index)\n",
        "\n",
        "            # Convert to tensor\n",
        "            input_tensor = torch.tensor(input_tensor_for_prediction, dtype=torch.long)\n",
        "            input_tensor = input_tensor.view(1, -1)\n",
        "\n",
        "            # Ensure the model is in evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Make predictions\n",
        "            with torch.no_grad():\n",
        "                output = self.model(input_tensor)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probabilities = torch.softmax(output, dim=1).numpy()\n",
        "\n",
        "            # Process the probabilities using int_to_char\n",
        "            probabilities_list = [(int_to_char[i], prob) for i, prob in enumerate(probabilities[0])]\n",
        "\n",
        "            alphabet_count = sum(1 for char in ngram if char.isalpha())\n",
        "\n",
        "            # Give more weight to the new built n-gram (new information)\n",
        "            if ngram not in self.ngram_used:\n",
        "                self.ngram_used.add(ngram)\n",
        "                weight = 2\n",
        "            else:\n",
        "                weight = 1\n",
        "\n",
        "            # Accumulate probabilities for each alphabet\n",
        "            for char, prob in probabilities_list:\n",
        "                accumulated_probabilities[char] = accumulated_probabilities.get(char, 0) + prob * weight\n",
        "\n",
        "        # Convert the accumulated probabilities to a list of tuples\n",
        "        final_accumulated_list = list(accumulated_probabilities.items())\n",
        "\n",
        "        sorted_probabilities_list = sorted(final_accumulated_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        sorted_letters_list = [pred for pred, _ in sorted_probabilities_list]\n",
        "\n",
        "        return sorted_letters_list\n",
        "\n",
        "\n",
        "    def guess(self, word):\n",
        "        # word input example: \"_ p p _ e \"\n",
        "\n",
        "        # clean the word so that we strip away the space characters\n",
        "        # replace \"_\" with \".\" as \".\" indicates any character in regular expressions\n",
        "        clean_word = word[::2].replace(\"_\",\".\")\n",
        "\n",
        "        # find length of passed word\n",
        "        len_word = len(clean_word)\n",
        "\n",
        "        # remaing spaces\n",
        "        remaining_spaces = clean_word.count('.')\n",
        "\n",
        "        # grab current dictionary of possible words from self object, initialize new possible words dictionary to empty\n",
        "        current_dictionary = self.full_dictionary\n",
        "        new_dictionary = []\n",
        "\n",
        "        # iterate through all of the words in the old plausible dictionary\n",
        "        for dict_word in current_dictionary:\n",
        "            # continue if the word is not of the appropriate length\n",
        "            if len(dict_word) != len_word:\n",
        "                continue\n",
        "\n",
        "            # if dictionary word is a possible match then add it to the current dictionary\n",
        "            if re.match(clean_word,dict_word):\n",
        "                new_dictionary.append(dict_word)\n",
        "\n",
        "        # overwrite old possible words dictionary with updated version\n",
        "        current_dictionary = new_dictionary\n",
        "\n",
        "        # start the guess letter\n",
        "        guess_letter = '!'\n",
        "\n",
        "        # if we have not yet guessed at least 2, start with most common letters,\n",
        "        # in the dictionary of the words with the same length\n",
        "        if (len_word - remaining_spaces) < 2:\n",
        "            full_dict_string = \"\".join(new_dictionary)\n",
        "            # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
        "            c = collections.Counter(full_dict_string)\n",
        "            sorted_letter_count = c.most_common()\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        remaining_sorted_letters = get_sorted_letters(new_dictionary, self.guessed_letters)\n",
        "\n",
        "        # now we have at least two letters, use LSTM:\n",
        "        if guess_letter == '!':\n",
        "            predict_letters = self.predicted_letter_lstm(word[::2])\n",
        "            for letter in predict_letters:\n",
        "                # check if the prediction is alphabet and it is not already suggested\n",
        "                if letter.isalpha() and letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "\n",
        "        # if there was no match: based on words with the same pattern:\n",
        "        if guess_letter == '!' or guess_letter == '_':\n",
        "            # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
        "            c = func(new_dictionary)\n",
        "            sorted_letter_count = c.most_common()\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        # if no word matches in training dictionary, default back to ordering of full dictionary\n",
        "        if guess_letter == '!' or guess_letter == '_':\n",
        "            sorted_letter_count = self.full_dictionary_common_letter_sorted\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        return guess_letter\n",
        "\n",
        "    ##########################################################\n",
        "    # You'll likely not need to modify any of the code below #\n",
        "    ##########################################################\n",
        "\n",
        "    def build_dictionary(self, dictionary_file_location):\n",
        "        text_file = open(dictionary_file_location,\"r\")\n",
        "        full_dictionary = text_file.read().splitlines()\n",
        "        text_file.close()\n",
        "        return full_dictionary\n",
        "\n",
        "    def start_game(self, practice=True, verbose=True):\n",
        "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
        "        self.guessed_letters = []\n",
        "        self.current_dictionary = self.full_dictionary\n",
        "\n",
        "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
        "        if response.get('status')==\"approved\":\n",
        "            game_id = response.get('game_id')\n",
        "            word = response.get('word')\n",
        "            self.tries_remains = response.get('tries_remains')\n",
        "            if verbose:\n",
        "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, self.tries_remains, word))\n",
        "            while self.tries_remains > 0:\n",
        "                # get guessed letter from user code\n",
        "                guess_letter = self.guess(word)\n",
        "\n",
        "                # append guessed letter to guessed letters field in hangman object\n",
        "                self.guessed_letters.append(guess_letter)\n",
        "                if verbose:\n",
        "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
        "\n",
        "                try:\n",
        "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
        "                except HangmanAPIError:\n",
        "                    print('HangmanAPIError exception caught on request.')\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print('Other exception caught on request.')\n",
        "                    raise e\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Sever response: {0}\".format(res))\n",
        "                status = res.get('status')\n",
        "                self.tries_remains = res.get('tries_remains')\n",
        "                if status==\"success\":\n",
        "                    if verbose:\n",
        "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
        "                    return True\n",
        "                elif status==\"failed\":\n",
        "                    reason = res.get('reason', '# of tries exceeded!')\n",
        "                    if verbose:\n",
        "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
        "                    return False\n",
        "                elif status==\"ongoing\":\n",
        "                    word = res.get('word')\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"Failed to start a new game\")\n",
        "        return status==\"success\"\n",
        "\n",
        "    def my_status(self):\n",
        "        return self.request(\"/my_status\", {})\n",
        "\n",
        "    def request(\n",
        "            self, path, args=None, post_args=None, method=None):\n",
        "        if args is None:\n",
        "            args = dict()\n",
        "        if post_args is not None:\n",
        "            method = \"POST\"\n",
        "\n",
        "        # Add `access_token` to post_args or args if it has not already been\n",
        "        # included.\n",
        "        if self.access_token:\n",
        "            # If post_args exists, we assume that args either does not exists\n",
        "            # or it does not need `access_token`.\n",
        "            if post_args and \"access_token\" not in post_args:\n",
        "                post_args[\"access_token\"] = self.access_token\n",
        "            elif \"access_token\" not in args:\n",
        "                args[\"access_token\"] = self.access_token\n",
        "\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        num_retry, time_sleep = 50, 2\n",
        "        for it in range(num_retry):\n",
        "            try:\n",
        "                response = self.session.request(\n",
        "                    method or \"GET\",\n",
        "                    self.hangman_url + path,\n",
        "                    timeout=self.timeout,\n",
        "                    params=args,\n",
        "                    data=post_args,\n",
        "                    verify=False\n",
        "                )\n",
        "                break\n",
        "            except requests.HTTPError as e:\n",
        "                response = json.loads(e.read())\n",
        "                raise HangmanAPIError(response)\n",
        "            except requests.exceptions.SSLError as e:\n",
        "                if it + 1 == num_retry:\n",
        "                    raise\n",
        "                time.sleep(time_sleep)\n",
        "\n",
        "        headers = response.headers\n",
        "        if 'json' in headers['content-type']:\n",
        "            result = response.json()\n",
        "        elif \"access_token\" in parse_qs(response.text):\n",
        "            query_str = parse_qs(response.text)\n",
        "            if \"access_token\" in query_str:\n",
        "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
        "                if \"expires\" in query_str:\n",
        "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
        "            else:\n",
        "                raise HangmanAPIError(response.json())\n",
        "        else:\n",
        "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
        "\n",
        "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
        "            raise HangmanAPIError(result)\n",
        "        return result\n",
        "\n",
        "class HangmanAPIError(Exception):\n",
        "    def __init__(self, result):\n",
        "        self.result = result\n",
        "        self.code = None\n",
        "        try:\n",
        "            self.type = result[\"error_code\"]\n",
        "        except (KeyError, TypeError):\n",
        "            self.type = \"\"\n",
        "\n",
        "        try:\n",
        "            self.message = result[\"error_description\"]\n",
        "        except (KeyError, TypeError):\n",
        "            try:\n",
        "                self.message = result[\"error\"][\"message\"]\n",
        "                self.code = result[\"error\"].get(\"code\")\n",
        "                if not self.type:\n",
        "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
        "            except (KeyError, TypeError):\n",
        "                try:\n",
        "                    self.message = result[\"error_msg\"]\n",
        "                except (KeyError, TypeError):\n",
        "                    self.message = result\n",
        "\n",
        "        Exception.__init__(self, self.message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtzEcX8_ppuk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
