{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Hangman Game\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9LEdV0s6cWF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from itertools import combinations\n",
        "import json\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "import secrets\n",
        "import time\n",
        "import re\n",
        "import collections\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "try:\n",
        "    from urllib.parse import parse_qs, urlencode, urlparse\n",
        "except ImportError:\n",
        "    from urlparse import parse_qs, urlparse\n",
        "    from urllib import urlencode\n",
        "\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "from utilities import *\n",
        "from model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsgDfkW1cowA",
        "outputId": "93e56c4f-2f1f-4d80-d4da-b5c2eaadccc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-gram is Done!\n",
            "3-gram is Done!\n",
            "4-gram is Done!\n",
            "5-gram is Done!\n",
            "6-gram is Done!\n",
            "Input data saved successfully.\n",
            "Target data saved successfully.\n",
            "torch.Size([29977280, 6]) torch.Size([29977280, 26])\n"
          ]
        }
      ],
      "source": [
        "# prepare the data for LSTM\n",
        "input_tensor, target_tensor = process_and_prepare_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMxocGfmiCO2",
        "outputId": "485cb3af-4166-449d-a8c1-20c73347e92a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            _________________________
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "train_model(input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7qFgX38Djy8"
      },
      "outputs": [],
      "source": [
        "def encode_input_for_prediction(masked_word):\n",
        "    \"\"\"\n",
        "    Encode the masked word.\n",
        "\n",
        "    Parameters:\n",
        "    - masked_word (str): The word with masked characters.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Encoded input tensor for prediction.\n",
        "    \"\"\"\n",
        "    # Create a character-to-index mapping and an underscore placeholder\n",
        "    char_to_index, _ = create_char_mapping()\n",
        "\n",
        "    # Encode the masked word using the char_to_index mapping\n",
        "    input_data = [encode_input(masked_word)]\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
        "    return input_tensor\n",
        "\n",
        "\n",
        "def extract_ngrams(word):\n",
        "    \"\"\"\n",
        "    Make all possible n-grams from the word with at least one underscore.\n",
        "\n",
        "    Parameters:\n",
        "    - word (str): The input word.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of unique n-grams.\n",
        "    \"\"\"\n",
        "    ngrams = set()\n",
        "\n",
        "    # Iterate over different n-gram lengths (2, 3, 4, 5, 6)\n",
        "    for n in range(2, 7):\n",
        "        # Extract n-grams from the word\n",
        "        for i in range(len(word) - n + 1):\n",
        "            ngram = word[i:i + n]\n",
        "\n",
        "            # Check if the n-gram contains at least one alphabet and one underscore\n",
        "            if any(char.isalpha() for char in ngram) and '_' in ngram:\n",
        "                # If the n-gram is shorter than 6, pad it with asterisks\n",
        "                ngram = ngram.ljust(6, '*')\n",
        "\n",
        "                # Ensure the n-gram is of length 6\n",
        "                ngram = ngram[:6]\n",
        "\n",
        "                # Add the n-gram to the set\n",
        "                ngrams.add(ngram)\n",
        "\n",
        "    return list(ngrams)\n",
        "\n",
        "def encode_ngram(ngram, char_to_index):\n",
        "    \"\"\"\n",
        "    Encode a given n-gram using a character-to-index mapping.\n",
        "\n",
        "    Parameters:\n",
        "    - ngram (str): The input n-gram.\n",
        "    - char_to_index (dict): Character-to-index mapping.\n",
        "\n",
        "    Returns:\n",
        "    - list: Encoded n-gram.\n",
        "    \"\"\"\n",
        "    # Ensure the n-gram is of length 6\n",
        "    ngram = ngram[:6]\n",
        "\n",
        "    # Encode each character in the n-gram using char_to_index mapping\n",
        "    encoded_ngram = [char_to_index[char] for char in ngram]\n",
        "\n",
        "    return encoded_ngram\n",
        "\n",
        "def get_sorted_letters(new_dictionary, guessed_letters):\n",
        "    \"\"\"\n",
        "    Get sorted letters based on their frequency in the new dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - new_dictionary (list): List of possible words.\n",
        "    - guessed_letters (list): List of guessed letters.\n",
        "\n",
        "    Returns:\n",
        "    - list: Sorted letters based on frequency, excluding guessed letters.\n",
        "    \"\"\"\n",
        "    full_dict_string = \"\".join(new_dictionary)\n",
        "\n",
        "    # Count the occurrences of each letter\n",
        "    c = collections.Counter(full_dict_string)\n",
        "\n",
        "    sorted_letter_count = c.most_common()\n",
        "\n",
        "    # Filter out guessed letters\n",
        "    remaining_sorted_letters = [item for item in sorted_letter_count if item[0] not in guessed_letters]\n",
        "\n",
        "    return remaining_sorted_letters\n",
        "\n",
        "def func(new_dictionary):\n",
        "    \"\"\"\n",
        "    Count the occurrences of each letter in the new dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - new_dictionary (list): List of possible words.\n",
        "\n",
        "    Returns:\n",
        "    - collections.Counter: Count of occurrences for each letter.\n",
        "    \"\"\"\n",
        "    dictx = collections.Counter()\n",
        "    for words in new_dictionary:\n",
        "        temp = collections.Counter(words)\n",
        "        for i in temp:\n",
        "            temp[i] = 1\n",
        "            dictx = dictx + temp\n",
        "    return dictx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dchQERaVHcpe"
      },
      "outputs": [],
      "source": [
        "class HangmanAPI(object):\n",
        "    def __init__(self, access_token=None, session=None, timeout=None):\n",
        "        self.hangman_url = self.determine_hangman_url()\n",
        "        self.access_token = access_token\n",
        "        self.session = session or requests.Session()\n",
        "        self.timeout = timeout\n",
        "        self.guessed_letters = []\n",
        "        full_dictionary_location = \"/content/words_250000_train.txt\"\n",
        "        self.full_dictionary = self.build_dictionary(full_dictionary_location)\n",
        "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
        "        self.current_dictionary = []\n",
        "        self.tries_remains  = 6\n",
        "        self.model = NeuralNetwork()\n",
        "        self.model.load_state_dict(torch.load(\"/content/lstm_ngram_2.pt\"))\n",
        "        self.ngram_used = set()\n",
        "        self.LSTM_guess = 0\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def determine_hangman_url():\n",
        "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
        "\n",
        "        data = {link: 0 for link in links}\n",
        "\n",
        "        for link in links:\n",
        "\n",
        "            requests.get(link)\n",
        "\n",
        "            for i in range(10):\n",
        "                s = time.time()\n",
        "                requests.get(link)\n",
        "                data[link] = time.time() - s\n",
        "\n",
        "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
        "        link += '/trexsim/hangman'\n",
        "        return link\n",
        "\n",
        "\n",
        "    def predicted_letter_lstm(self, masked_word):\n",
        "        \"\"\"\n",
        "        Predict the next letter using the LSTM model based on the masked word.\n",
        "\n",
        "        Parameters:\n",
        "        - masked_word (str): The word with masked characters.\n",
        "\n",
        "        Returns:\n",
        "        - list: Predicted letters sorted by probability.\n",
        "        \"\"\"\n",
        "        # Create character mappings\n",
        "        char_to_index, int_to_char = create_char_mapping()\n",
        "\n",
        "        # Extract unique n-grams with at least one alphabet\n",
        "        ngrams = extract_ngrams(masked_word)\n",
        "\n",
        "        # Initialize an empty dictionary to store accumulated probabilities\n",
        "        accumulated_probabilities = {}\n",
        "\n",
        "        # Traverse over n-grams\n",
        "        for ngram in ngrams:\n",
        "            # Encode the n-gram\n",
        "            input_tensor_for_prediction = encode_ngram(ngram, char_to_index)\n",
        "\n",
        "            # Convert to tensor\n",
        "            input_tensor = torch.tensor(input_tensor_for_prediction, dtype=torch.long)\n",
        "            input_tensor = input_tensor.view(1, -1)\n",
        "\n",
        "            # Ensure the model is in evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Make predictions\n",
        "            with torch.no_grad():\n",
        "                output = self.model(input_tensor)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probabilities = torch.softmax(output, dim=1).numpy()\n",
        "\n",
        "            # Process the probabilities using int_to_char\n",
        "            probabilities_list = [(int_to_char[i], prob) for i, prob in enumerate(probabilities[0])]\n",
        "\n",
        "            alphabet_count = sum(1 for char in ngram if char.isalpha())\n",
        "\n",
        "            # Give more weight to the new built n-gram (new information)\n",
        "            if ngram not in self.ngram_used:\n",
        "                self.ngram_used.add(ngram)\n",
        "                weight = 2\n",
        "            else:\n",
        "                weight = 1\n",
        "\n",
        "            # Accumulate probabilities for each alphabet\n",
        "            for char, prob in probabilities_list:\n",
        "                accumulated_probabilities[char] = accumulated_probabilities.get(char, 0) + prob * weight\n",
        "\n",
        "        # Convert the accumulated probabilities to a list of tuples\n",
        "        final_accumulated_list = list(accumulated_probabilities.items())\n",
        "\n",
        "        sorted_probabilities_list = sorted(final_accumulated_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        sorted_letters_list = [pred for pred, _ in sorted_probabilities_list]\n",
        "\n",
        "        return sorted_letters_list\n",
        "\n",
        "\n",
        "    def guess(self, word):\n",
        "        # word input example: \"_ p p _ e \"\n",
        "\n",
        "        # clean the word so that we strip away the space characters\n",
        "        # replace \"_\" with \".\" as \".\" indicates any character in regular expressions\n",
        "        clean_word = word[::2].replace(\"_\",\".\")\n",
        "\n",
        "        # find length of passed word\n",
        "        len_word = len(clean_word)\n",
        "\n",
        "        # remaing spaces\n",
        "        remaining_spaces = clean_word.count('.')\n",
        "\n",
        "        # grab current dictionary of possible words from self object, initialize new possible words dictionary to empty\n",
        "        current_dictionary = self.full_dictionary\n",
        "        new_dictionary = []\n",
        "\n",
        "        # iterate through all of the words in the old plausible dictionary\n",
        "        for dict_word in current_dictionary:\n",
        "            # continue if the word is not of the appropriate length\n",
        "            if len(dict_word) != len_word:\n",
        "                continue\n",
        "\n",
        "            # if dictionary word is a possible match then add it to the current dictionary\n",
        "            if re.match(clean_word,dict_word):\n",
        "                new_dictionary.append(dict_word)\n",
        "\n",
        "        # overwrite old possible words dictionary with updated version\n",
        "        current_dictionary = new_dictionary\n",
        "\n",
        "        # start the guess letter\n",
        "        guess_letter = '!'\n",
        "\n",
        "        # if we have not yet guessed at least 2, start with most common letters,\n",
        "        # in the dictionary of the words with the same length\n",
        "        if (len_word - remaining_spaces) < 2:\n",
        "            full_dict_string = \"\".join(new_dictionary)\n",
        "            # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
        "            c = collections.Counter(full_dict_string)\n",
        "            sorted_letter_count = c.most_common()\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        remaining_sorted_letters = get_sorted_letters(new_dictionary, self.guessed_letters)\n",
        "\n",
        "        # now we have at least two letters, use LSTM:\n",
        "        if guess_letter == '!':\n",
        "            predict_letters = self.predicted_letter_lstm(word[::2])\n",
        "            for letter in predict_letters:\n",
        "                # check if the prediction is alphabet and it is not already suggested\n",
        "                if letter.isalpha() and letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "\n",
        "        # if there was no match: based on words with the same pattern:\n",
        "        if guess_letter == '!' or guess_letter == '_':\n",
        "            # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
        "            c = func(new_dictionary)\n",
        "            sorted_letter_count = c.most_common()\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        # if no word matches in training dictionary, default back to ordering of full dictionary\n",
        "        if guess_letter == '!' or guess_letter == '_':\n",
        "            sorted_letter_count = self.full_dictionary_common_letter_sorted\n",
        "            for letter,_ in sorted_letter_count:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    guess_letter = letter\n",
        "                    break\n",
        "\n",
        "        return guess_letter\n",
        "\n",
        "    ##########################################################\n",
        "    # You'll likely not need to modify any of the code below #\n",
        "    ##########################################################\n",
        "\n",
        "    def build_dictionary(self, dictionary_file_location):\n",
        "        text_file = open(dictionary_file_location,\"r\")\n",
        "        full_dictionary = text_file.read().splitlines()\n",
        "        text_file.close()\n",
        "        return full_dictionary\n",
        "\n",
        "    def start_game(self, practice=True, verbose=True):\n",
        "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
        "        self.guessed_letters = []\n",
        "        self.current_dictionary = self.full_dictionary\n",
        "\n",
        "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
        "        if response.get('status')==\"approved\":\n",
        "            game_id = response.get('game_id')\n",
        "            word = response.get('word')\n",
        "            self.tries_remains = response.get('tries_remains')\n",
        "            if verbose:\n",
        "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, self.tries_remains, word))\n",
        "            while self.tries_remains > 0:\n",
        "                # get guessed letter from user code\n",
        "                guess_letter = self.guess(word)\n",
        "\n",
        "                # append guessed letter to guessed letters field in hangman object\n",
        "                self.guessed_letters.append(guess_letter)\n",
        "                if verbose:\n",
        "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
        "\n",
        "                try:\n",
        "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
        "                except HangmanAPIError:\n",
        "                    print('HangmanAPIError exception caught on request.')\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print('Other exception caught on request.')\n",
        "                    raise e\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Sever response: {0}\".format(res))\n",
        "                status = res.get('status')\n",
        "                self.tries_remains = res.get('tries_remains')\n",
        "                if status==\"success\":\n",
        "                    if verbose:\n",
        "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
        "                    return True\n",
        "                elif status==\"failed\":\n",
        "                    reason = res.get('reason', '# of tries exceeded!')\n",
        "                    if verbose:\n",
        "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
        "                    return False\n",
        "                elif status==\"ongoing\":\n",
        "                    word = res.get('word')\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"Failed to start a new game\")\n",
        "        return status==\"success\"\n",
        "\n",
        "    def my_status(self):\n",
        "        return self.request(\"/my_status\", {})\n",
        "\n",
        "    def request(\n",
        "            self, path, args=None, post_args=None, method=None):\n",
        "        if args is None:\n",
        "            args = dict()\n",
        "        if post_args is not None:\n",
        "            method = \"POST\"\n",
        "\n",
        "        # Add `access_token` to post_args or args if it has not already been\n",
        "        # included.\n",
        "        if self.access_token:\n",
        "            # If post_args exists, we assume that args either does not exists\n",
        "            # or it does not need `access_token`.\n",
        "            if post_args and \"access_token\" not in post_args:\n",
        "                post_args[\"access_token\"] = self.access_token\n",
        "            elif \"access_token\" not in args:\n",
        "                args[\"access_token\"] = self.access_token\n",
        "\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        num_retry, time_sleep = 50, 2\n",
        "        for it in range(num_retry):\n",
        "            try:\n",
        "                response = self.session.request(\n",
        "                    method or \"GET\",\n",
        "                    self.hangman_url + path,\n",
        "                    timeout=self.timeout,\n",
        "                    params=args,\n",
        "                    data=post_args,\n",
        "                    verify=False\n",
        "                )\n",
        "                break\n",
        "            except requests.HTTPError as e:\n",
        "                response = json.loads(e.read())\n",
        "                raise HangmanAPIError(response)\n",
        "            except requests.exceptions.SSLError as e:\n",
        "                if it + 1 == num_retry:\n",
        "                    raise\n",
        "                time.sleep(time_sleep)\n",
        "\n",
        "        headers = response.headers\n",
        "        if 'json' in headers['content-type']:\n",
        "            result = response.json()\n",
        "        elif \"access_token\" in parse_qs(response.text):\n",
        "            query_str = parse_qs(response.text)\n",
        "            if \"access_token\" in query_str:\n",
        "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
        "                if \"expires\" in query_str:\n",
        "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
        "            else:\n",
        "                raise HangmanAPIError(response.json())\n",
        "        else:\n",
        "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
        "\n",
        "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
        "            raise HangmanAPIError(result)\n",
        "        return result\n",
        "\n",
        "class HangmanAPIError(Exception):\n",
        "    def __init__(self, result):\n",
        "        self.result = result\n",
        "        self.code = None\n",
        "        try:\n",
        "            self.type = result[\"error_code\"]\n",
        "        except (KeyError, TypeError):\n",
        "            self.type = \"\"\n",
        "\n",
        "        try:\n",
        "            self.message = result[\"error_description\"]\n",
        "        except (KeyError, TypeError):\n",
        "            try:\n",
        "                self.message = result[\"error\"][\"message\"]\n",
        "                self.code = result[\"error\"].get(\"code\")\n",
        "                if not self.type:\n",
        "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
        "            except (KeyError, TypeError):\n",
        "                try:\n",
        "                    self.message = result[\"error_msg\"]\n",
        "                except (KeyError, TypeError):\n",
        "                    self.message = result\n",
        "\n",
        "        Exception.__init__(self, self.message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtzEcX8_ppuk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
